"""
Performance Optimization System
æ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿ - ä¸ºå¤§è§„æ¨¡æ•°æ®é›†æä¾›é«˜æ•ˆå¤„ç†èƒ½åŠ›
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Iterator, Generator
import json
import tempfile
import os
from pathlib import Path
import hashlib
import pickle
import gc
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import psutil
from functools import lru_cache, wraps
from dataclasses import dataclass
from datetime import datetime, timedelta
import sqlite3

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    memory_usage_mb: float
    processing_time_s: float
    cpu_usage_percent: float
    cache_hit_rate: float
    data_size_mb: float
    optimization_applied: List[str]

@dataclass
class DataChunk:
    """æ•°æ®åˆ†å—"""
    chunk_id: str
    start_idx: int
    end_idx: int
    size: int
    data_hash: str
    cached: bool = False

class MemoryManager:
    """å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, max_memory_gb: float = 8.0):
        self.max_memory_bytes = max_memory_gb * 1024 * 1024 * 1024
        self.current_usage = 0
        self.tracked_objects = {}
        
    def get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆMBï¼‰"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    
    def check_memory_pressure(self) -> bool:
        """æ£€æŸ¥å†…å­˜å‹åŠ›"""
        current_mb = self.get_memory_usage()
        threshold_mb = (self.max_memory_bytes / 1024 / 1024) * 0.8  # 80% é˜ˆå€¼
        return current_mb > threshold_mb
    
    def suggest_chunk_size(self, data_size_mb: float, target_chunks: int = 4) -> int:
        """å»ºè®®åˆ†å—å¤§å°"""
        available_mb = (self.max_memory_bytes / 1024 / 1024) * 0.6  # 60% å¯ç”¨
        chunk_size_mb = min(available_mb / target_chunks, data_size_mb / target_chunks)
        
        # è½¬æ¢ä¸ºè¡Œæ•°ä¼°ç®—ï¼ˆå‡è®¾æ¯è¡Œå¹³å‡ 1KBï¼‰
        estimated_rows = int(chunk_size_mb * 1024)
        return max(1000, estimated_rows)  # æœ€å° 1000 è¡Œ

class IntelligentCache:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, cache_dir: str = "cache", max_size_gb: float = 10.0):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.max_size_bytes = max_size_gb * 1024 * 1024 * 1024
        self.metadata_db = self.cache_dir / "cache_metadata.db"
        self._init_database()
        
    def _init_database(self):
        """åˆå§‹åŒ–ç¼“å­˜å…ƒæ•°æ®æ•°æ®åº“"""
        conn = sqlite3.connect(self.metadata_db)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS cache_entries (
                key TEXT PRIMARY KEY,
                file_path TEXT,
                size_bytes INTEGER,
                created_at TIMESTAMP,
                last_accessed TIMESTAMP,
                access_count INTEGER DEFAULT 1,
                data_type TEXT
            )
        """)
        conn.commit()
        conn.close()
    
    def _get_cache_key(self, data_hash: str, operation: str, params: Dict = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{data_hash}_{operation}"
        if params:
            key_data += "_" + hashlib.md5(json.dumps(params, sort_keys=True).encode()).hexdigest()
        return hashlib.sha256(key_data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        conn = sqlite3.connect(self.metadata_db)
        cursor = conn.execute(
            "SELECT file_path FROM cache_entries WHERE key = ?", (key,)
        )
        result = cursor.fetchone()
        conn.close()
        
        if result:
            file_path = Path(result[0])
            if file_path.exists():
                # æ›´æ–°è®¿é—®æ—¶é—´å’Œè®¡æ•°
                self._update_access_stats(key)
                
                try:
                    with open(file_path, 'rb') as f:
                        return pickle.load(f)
                except:
                    # ç¼“å­˜æ–‡ä»¶æŸåï¼Œåˆ é™¤
                    self._remove_entry(key)
                    return None
        
        return None
    
    def set(self, key: str, data: Any, data_type: str = "general") -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        try:
            # æ£€æŸ¥ç¼“å­˜ç©ºé—´
            if not self._ensure_cache_space():
                return False
            
            # ä¿å­˜æ•°æ®
            file_path = self.cache_dir / f"{key}.pkl"
            with open(file_path, 'wb') as f:
                pickle.dump(data, f)
            
            # è®°å½•å…ƒæ•°æ®
            size_bytes = file_path.stat().st_size
            now = datetime.now()
            
            conn = sqlite3.connect(self.metadata_db)
            conn.execute("""
                INSERT OR REPLACE INTO cache_entries 
                (key, file_path, size_bytes, created_at, last_accessed, data_type)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (key, str(file_path), size_bytes, now, now, data_type))
            conn.commit()
            conn.close()
            
            return True
            
        except Exception as e:
            print(f"Cache set error: {e}")
            return False
    
    def _ensure_cache_space(self) -> bool:
        """ç¡®ä¿ç¼“å­˜ç©ºé—´è¶³å¤Ÿ"""
        current_size = self._get_cache_size()
        
        if current_size > self.max_size_bytes * 0.9:  # 90% é˜ˆå€¼
            # æ¸…ç†æœ€å°‘ä½¿ç”¨çš„ç¼“å­˜
            self._cleanup_lru_entries(0.3)  # æ¸…ç† 30% çš„ç©ºé—´
        
        return True
    
    def _get_cache_size(self) -> int:
        """è·å–å½“å‰ç¼“å­˜å¤§å°"""
        conn = sqlite3.connect(self.metadata_db)
        cursor = conn.execute("SELECT SUM(size_bytes) FROM cache_entries")
        result = cursor.fetchone()
        conn.close()
        return result[0] or 0
    
    def _cleanup_lru_entries(self, cleanup_ratio: float = 0.3):
        """æ¸…ç†æœ€å°‘ä½¿ç”¨çš„æ¡ç›®"""
        conn = sqlite3.connect(self.metadata_db)
        
        # æŒ‰è®¿é—®æ—¶é—´å’Œé¢‘ç‡æ’åºï¼Œåˆ é™¤æœ€å°‘ä½¿ç”¨çš„
        cursor = conn.execute("""
            SELECT key, file_path FROM cache_entries 
            ORDER BY access_count ASC, last_accessed ASC
        """)
        
        all_entries = cursor.fetchall()
        cleanup_count = int(len(all_entries) * cleanup_ratio)
        
        for key, file_path in all_entries[:cleanup_count]:
            try:
                Path(file_path).unlink(missing_ok=True)
                conn.execute("DELETE FROM cache_entries WHERE key = ?", (key,))
            except:
                continue
        
        conn.commit()
        conn.close()
    
    def _update_access_stats(self, key: str):
        """æ›´æ–°è®¿é—®ç»Ÿè®¡"""
        conn = sqlite3.connect(self.metadata_db)
        conn.execute("""
            UPDATE cache_entries 
            SET last_accessed = ?, access_count = access_count + 1
            WHERE key = ?
        """, (datetime.now(), key))
        conn.commit()
        conn.close()
    
    def _remove_entry(self, key: str):
        """åˆ é™¤ç¼“å­˜æ¡ç›®"""
        conn = sqlite3.connect(self.metadata_db)
        cursor = conn.execute("SELECT file_path FROM cache_entries WHERE key = ?", (key,))
        result = cursor.fetchone()
        
        if result:
            Path(result[0]).unlink(missing_ok=True)
            conn.execute("DELETE FROM cache_entries WHERE key = ?", (key,))
        
        conn.commit()
        conn.close()

class DataChunker:
    """æ•°æ®åˆ†å—å¤„ç†å™¨"""
    
    def __init__(self, memory_manager: MemoryManager):
        self.memory_manager = memory_manager
        
    def create_chunks(self, data: pd.DataFrame, max_chunk_size: int = None) -> List[DataChunk]:
        """åˆ›å»ºæ•°æ®åˆ†å—"""
        if max_chunk_size is None:
            data_size_mb = data.memory_usage(deep=True).sum() / 1024 / 1024
            max_chunk_size = self.memory_manager.suggest_chunk_size(data_size_mb)
        
        chunks = []
        total_rows = len(data)
        
        for start_idx in range(0, total_rows, max_chunk_size):
            end_idx = min(start_idx + max_chunk_size, total_rows)
            chunk_data = data.iloc[start_idx:end_idx]
            
            # ç”Ÿæˆæ•°æ®å“ˆå¸Œ
            data_hash = hashlib.md5(
                pd.util.hash_pandas_object(chunk_data).values
            ).hexdigest()
            
            chunk = DataChunk(
                chunk_id=f"chunk_{start_idx}_{end_idx}",
                start_idx=start_idx,
                end_idx=end_idx,
                size=end_idx - start_idx,
                data_hash=data_hash
            )
            
            chunks.append(chunk)
        
        return chunks
    
    def process_chunks_parallel(self, chunks: List[DataChunk], data: pd.DataFrame,
                               processing_func: callable, max_workers: int = None) -> List[Any]:
        """å¹¶è¡Œå¤„ç†æ•°æ®åˆ†å—"""
        if max_workers is None:
            max_workers = min(4, os.cpu_count() or 1)
        
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = []
            for chunk in chunks:
                chunk_data = data.iloc[chunk.start_idx:chunk.end_idx]
                future = executor.submit(processing_func, chunk_data, chunk.chunk_id)
                futures.append((future, chunk))
            
            # æ”¶é›†ç»“æœ
            for future, chunk in futures:
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    print(f"Error processing chunk {chunk.chunk_id}: {e}")
                    results.append(None)
        
        return results

class StreamingProcessor:
    """æµå¼æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, cache: IntelligentCache):
        self.cache = cache
        
    def stream_process_large_matrix(self, file_path: str, 
                                   processing_func: callable,
                                   chunk_size: int = 10000) -> Iterator[Any]:
        """æµå¼å¤„ç†å¤§å‹çŸ©é˜µæ–‡ä»¶"""
        
        file_hash = self._get_file_hash(file_path)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„å¤„ç†ç»“æœ
        cache_key = self._get_cache_key(file_hash, processing_func.__name__)
        cached_result = self.cache.get(cache_key)
        
        if cached_result:
            yield from cached_result
            return
        
        # æµå¼è¯»å–å’Œå¤„ç†
        results = []
        
        try:
            # ä½¿ç”¨ pandas çš„åˆ†å—è¯»å–
            chunk_reader = pd.read_csv(file_path, chunksize=chunk_size)
            
            for chunk_idx, chunk in enumerate(chunk_reader):
                try:
                    # å¤„ç†å½“å‰åˆ†å—
                    processed_chunk = processing_func(chunk)
                    results.append(processed_chunk)
                    
                    yield processed_chunk
                    
                    # å†…å­˜ç®¡ç†
                    if chunk_idx % 10 == 0:  # æ¯ 10 ä¸ªåˆ†å—æ£€æŸ¥ä¸€æ¬¡
                        if self.memory_manager.check_memory_pressure():
                            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
                            
                except Exception as e:
                    print(f"Error processing chunk {chunk_idx}: {e}")
                    continue
            
            # ç¼“å­˜å®Œæ•´ç»“æœ
            self.cache.set(cache_key, results, "stream_processing")
            
        except Exception as e:
            print(f"Error in stream processing: {e}")
    
    def _get_file_hash(self, file_path: str) -> str:
        """è·å–æ–‡ä»¶å“ˆå¸Œï¼ˆä»…è¯»å–éƒ¨åˆ†å†…å®¹ä»¥æé«˜é€Ÿåº¦ï¼‰"""
        hash_obj = hashlib.md5()
        
        with open(file_path, 'rb') as f:
            # è¯»å–æ–‡ä»¶å¼€å¤´å’Œç»“å°¾çš„éƒ¨åˆ†å†…å®¹
            hash_obj.update(f.read(8192))  # å‰ 8KB
            
            f.seek(-8192, 2)  # ä»æ–‡ä»¶æœ«å°¾å¼€å§‹
            hash_obj.update(f.read(8192))  # å 8KB
            
        return hash_obj.hexdigest()
    
    def _get_cache_key(self, file_hash: str, operation: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{file_hash}_{operation}"

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨ä¸»ç±»"""
    
    def __init__(self, max_memory_gb: float = 8.0, cache_size_gb: float = 10.0):
        self.memory_manager = MemoryManager(max_memory_gb)
        self.cache = IntelligentCache(cache_size_gb=cache_size_gb)
        self.chunker = DataChunker(self.memory_manager)
        self.streaming_processor = StreamingProcessor(self.cache)
        self.performance_metrics = []
        
    def optimize_data_loading(self, file_path: str, **kwargs) -> pd.DataFrame:
        """ä¼˜åŒ–æ•°æ®åŠ è½½"""
        start_time = datetime.now()
        start_memory = self.memory_manager.get_memory_usage()
        optimizations_applied = []
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size_mb = Path(file_path).stat().st_size / 1024 / 1024
        
        if file_size_mb > 500:  # å¤§äº 500MB
            optimizations_applied.append("chunked_loading")
            
            # åˆ†å—åŠ è½½
            chunks = []
            chunk_size = self.memory_manager.suggest_chunk_size(file_size_mb)
            
            for chunk in pd.read_csv(file_path, chunksize=chunk_size, **kwargs):
                chunks.append(chunk)
                
                # å†…å­˜æ£€æŸ¥
                if self.memory_manager.check_memory_pressure():
                    gc.collect()
            
            data = pd.concat(chunks, ignore_index=True)
            del chunks  # æ¸…ç†ä¸­é—´å˜é‡
            
        else:
            # ç›´æ¥åŠ è½½
            data = pd.read_csv(file_path, **kwargs)
        
        # å†…å­˜ä¼˜åŒ–
        if file_size_mb > 100:  # å¤§äº 100MB æ‰è¿›è¡Œä¼˜åŒ–
            data = self._optimize_dataframe_memory(data)
            optimizations_applied.append("memory_optimization")
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        end_time = datetime.now()
        end_memory = self.memory_manager.get_memory_usage()
        
        metrics = PerformanceMetrics(
            memory_usage_mb=end_memory - start_memory,
            processing_time_s=(end_time - start_time).total_seconds(),
            cpu_usage_percent=psutil.cpu_percent(),
            cache_hit_rate=0.0,  # æ–°åŠ è½½çš„æ•°æ®
            data_size_mb=file_size_mb,
            optimization_applied=optimizations_applied
        )
        
        self.performance_metrics.append(metrics)
        
        return data
    
    def optimize_computation(self, computation_func: callable, 
                           data: pd.DataFrame, *args, **kwargs) -> Any:
        """ä¼˜åŒ–è®¡ç®—è¿‡ç¨‹"""
        # ç”Ÿæˆç¼“å­˜é”®
        data_hash = hashlib.md5(pd.util.hash_pandas_object(data).values).hexdigest()
        cache_key = f"{data_hash}_{computation_func.__name__}_{hash(str(args) + str(kwargs))}"
        
        # æ£€æŸ¥ç¼“å­˜
        cached_result = self.cache.get(cache_key)
        if cached_result is not None:
            return cached_result
        
        start_time = datetime.now()
        optimizations_applied = []
        
        # æ£€æŸ¥æ•°æ®å¤§å°å†³å®šä¼˜åŒ–ç­–ç•¥
        data_size_mb = data.memory_usage(deep=True).sum() / 1024 / 1024
        
        if data_size_mb > 200:  # å¤§æ•°æ®é›†ä½¿ç”¨åˆ†å—å¤„ç†
            optimizations_applied.append("chunked_computation")
            
            chunks = self.chunker.create_chunks(data)
            chunk_results = self.chunker.process_chunks_parallel(
                chunks, data, 
                lambda chunk_data, chunk_id: computation_func(chunk_data, *args, **kwargs)
            )
            
            # åˆå¹¶ç»“æœï¼ˆæ ¹æ®å‡½æ•°ç±»å‹ï¼‰
            if hasattr(chunk_results[0], 'shape'):  # DataFrame/Array
                result = pd.concat(chunk_results, ignore_index=True)
            elif isinstance(chunk_results[0], dict):  # å­—å…¸ç»“æœ
                result = self._merge_dict_results(chunk_results)
            else:  # å…¶ä»–ç±»å‹
                result = chunk_results
                
        else:
            # ç›´æ¥è®¡ç®—
            result = computation_func(data, *args, **kwargs)
        
        # ç¼“å­˜ç»“æœ
        self.cache.set(cache_key, result, "computation")
        
        # è®°å½•æ€§èƒ½
        end_time = datetime.now()
        metrics = PerformanceMetrics(
            memory_usage_mb=self.memory_manager.get_memory_usage(),
            processing_time_s=(end_time - start_time).total_seconds(),
            cpu_usage_percent=psutil.cpu_percent(),
            cache_hit_rate=0.0,
            data_size_mb=data_size_mb,
            optimization_applied=optimizations_applied
        )
        
        self.performance_metrics.append(metrics)
        
        return result
    
    def _optimize_dataframe_memory(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¼˜åŒ– DataFrame å†…å­˜ä½¿ç”¨"""
        optimized_df = df.copy()
        
        for col in optimized_df.columns:
            col_type = optimized_df[col].dtype
            
            if col_type == 'object':
                # å°è¯•è½¬æ¢ä¸ºåˆ†ç±»æ•°æ®
                if optimized_df[col].nunique() / len(optimized_df) < 0.5:  # 50% é˜ˆå€¼
                    optimized_df[col] = optimized_df[col].astype('category')
            
            elif col_type.name.startswith('int'):
                # ä¼˜åŒ–æ•´æ•°ç±»å‹
                col_min = optimized_df[col].min()
                col_max = optimized_df[col].max()
                
                if col_min >= 0:
                    if col_max < 255:
                        optimized_df[col] = optimized_df[col].astype(np.uint8)
                    elif col_max < 65535:
                        optimized_df[col] = optimized_df[col].astype(np.uint16)
                    elif col_max < 4294967295:
                        optimized_df[col] = optimized_df[col].astype(np.uint32)
                else:
                    if col_min > -128 and col_max < 127:
                        optimized_df[col] = optimized_df[col].astype(np.int8)
                    elif col_min > -32768 and col_max < 32767:
                        optimized_df[col] = optimized_df[col].astype(np.int16)
                    elif col_min > -2147483648 and col_max < 2147483647:
                        optimized_df[col] = optimized_df[col].astype(np.int32)
            
            elif col_type.name.startswith('float'):
                # ä¼˜åŒ–æµ®ç‚¹ç±»å‹
                optimized_df[col] = pd.to_numeric(optimized_df[col], downcast='float')
        
        return optimized_df
    
    def _merge_dict_results(self, dict_results: List[Dict]) -> Dict:
        """åˆå¹¶å­—å…¸ç±»å‹çš„ç»“æœ"""
        if not dict_results:
            return {}
        
        merged = {}
        
        for key in dict_results[0].keys():
            values = [result[key] for result in dict_results if key in result]
            
            if isinstance(values[0], (int, float)):
                merged[key] = sum(values) / len(values)  # å¹³å‡å€¼
            elif isinstance(values[0], list):
                merged[key] = [item for sublist in values for item in sublist]  # åˆå¹¶åˆ—è¡¨
            elif isinstance(values[0], pd.DataFrame):
                merged[key] = pd.concat(values, ignore_index=True)
            else:
                merged[key] = values  # ä¿æŒåŸå§‹åˆ—è¡¨
        
        return merged
    
    def get_performance_report(self) -> Dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        if not self.performance_metrics:
            return {'message': 'No performance data available'}
        
        recent_metrics = self.performance_metrics[-10:]  # æœ€è¿‘ 10 æ¬¡
        
        return {
            'total_operations': len(self.performance_metrics),
            'recent_operations': len(recent_metrics),
            'average_memory_usage_mb': sum(m.memory_usage_mb for m in recent_metrics) / len(recent_metrics),
            'average_processing_time_s': sum(m.processing_time_s for m in recent_metrics) / len(recent_metrics),
            'cache_statistics': self._get_cache_statistics(),
            'optimization_frequency': self._get_optimization_frequency(),
            'recommendations': self._generate_performance_recommendations()
        }
    
    def _get_cache_statistics(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        conn = sqlite3.connect(self.cache.metadata_db)
        
        # æ€»æ¡ç›®æ•°
        cursor = conn.execute("SELECT COUNT(*) FROM cache_entries")
        total_entries = cursor.fetchone()[0]
        
        # æ€»å¤§å°
        cursor = conn.execute("SELECT SUM(size_bytes) FROM cache_entries")
        total_size = cursor.fetchone()[0] or 0
        
        # å¹³å‡è®¿é—®æ¬¡æ•°
        cursor = conn.execute("SELECT AVG(access_count) FROM cache_entries")
        avg_access = cursor.fetchone()[0] or 0
        
        conn.close()
        
        return {
            'total_entries': total_entries,
            'total_size_mb': total_size / 1024 / 1024,
            'average_access_count': round(avg_access, 2),
            'cache_utilization': (total_size / self.cache.max_size_bytes) * 100
        }
    
    def _get_optimization_frequency(self) -> Dict:
        """è·å–ä¼˜åŒ–é¢‘ç‡ç»Ÿè®¡"""
        optimization_counts = {}
        
        for metrics in self.performance_metrics:
            for opt in metrics.optimization_applied:
                optimization_counts[opt] = optimization_counts.get(opt, 0) + 1
        
        return optimization_counts
    
    def _generate_performance_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½å»ºè®®"""
        recommendations = []
        
        if not self.performance_metrics:
            return recommendations
        
        recent_metrics = self.performance_metrics[-5:]
        avg_memory = sum(m.memory_usage_mb for m in recent_metrics) / len(recent_metrics)
        avg_time = sum(m.processing_time_s for m in recent_metrics) / len(recent_metrics)
        
        if avg_memory > 1000:  # è¶…è¿‡ 1GB
            recommendations.append("ğŸ’¾ å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®å¯ç”¨æ›´å¤šæ•°æ®åˆ†å—å¤„ç†")
        
        if avg_time > 60:  # è¶…è¿‡ 1 åˆ†é’Ÿ
            recommendations.append("âš¡ å¤„ç†æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä½¿ç”¨å¹¶è¡Œå¤„ç†æˆ–ç¼“å­˜")
        
        cache_stats = self._get_cache_statistics()
        if cache_stats['cache_utilization'] > 90:
            recommendations.append("ğŸ—„ï¸ ç¼“å­˜æ¥è¿‘æ»¡è½½ï¼Œå»ºè®®æ¸…ç†æˆ–å¢åŠ ç¼“å­˜ç©ºé—´")
        
        optimization_counts = self._get_optimization_frequency()
        if optimization_counts.get('chunked_processing', 0) < 2:
            recommendations.append("ğŸ”§ è€ƒè™‘ä¸ºå¤§æ•°æ®é›†å¯ç”¨åˆ†å—å¤„ç†ä¼˜åŒ–")
        
        return recommendations

# å…¨å±€å®ä¾‹
performance_optimizer = PerformanceOptimizer()